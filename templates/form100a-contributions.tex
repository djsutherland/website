{% raw -%}
\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[american]{babel}
\usepackage{csquotes}

\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage[margin=0.75in,headheight=13.6pt]{geometry}
\usepackage{times}
\usepackage[compact]{titlesec}

\usepackage{todonotes}
% \usepackage{hyperref}

\usepackage[capitalize,noabbrev]{cleveref}

\usepackage[maxbibnames=20,doi=false,url=false,sorting=ydnt,style=authortitle,giveninits=true,eprint=false,uniquelist=false,dashed=false]{biblatex}
\DeclareNameAlias{author}{given-family}

% \DeclareFieldFormat[unpublished,misc]{title}{\mkbibquote{#1\isdot}}
% \DeclareFieldFormat[unpublished,misc]{labeltitle}{\mkbibquote{#1}}
\renewbibmacro{in:}{}

% cite by title only
\renewbibmacro{cite}{\usebibmacro{cite:title}}

\newcommand{\bibeqcon}{{\footnotemark[1]}}

% bold student names in bib
\renewcommand*{\mkbibnamegiven}[1]{%
  \ifitemannotation{me}%
    {#1}%{\textbf{#1}}%
    {\ifitemannotation{mystudent}{\textbf{#1}}{#1}}}
\renewcommand*{\mkbibnamefamily}[1]{%
  \ifitemannotation{me}%
    {#1}%{\textbf{#1}}%
    {\ifitemannotation{mystudent}{\textbf{#1}}{#1}}%
  \ifitemannotation{equal}
    {\bibeqcon}
    {}%
}

{% endraw %}

{%- set sections = [
  ["Journal papers", ["journal"], None],
  ["Low-acceptance-rate conference papers", ["conference"], None],
  ["Papers under submission to journals and low-acceptance-rate conferences", ["preprint", "private"], True],
  ["High-acceptance-rate and non-refereed contributions", ["workshop", "poster", "tech-report", "preprint"], False],
] -%}

{% for sec_name, sec_types, submitted_only in sections %}
  \DeclareBibliographyCategory{sec{{ loop.index }}}
  \addtocategory{sec{{ loop.index -}} }{
    {%- for paper in venue_type_map[sec_types]
        if paper|in_last_years(6) and not paper.full_version
           and (submitted_only is none or (paper.submitted and submitted_only or not paper.submitted and not submitted_only))
    -%}
      {{- paper | bibtex_key(coauthors) -}}
      {%- if not loop.last -%},{%- endif -%}
    {%- endfor -%}
  }
{% endfor %}

{% raw %}

\DeclareSourcemap{
  \maps[datatype=bibtex]{
    \map[overwrite]{
      \step[fieldset=pubstate, null]
    }
    % % arxiv info only if there's no venue
    % \map[overwrite]{
    %   \pertype{article}
    %   \step[fieldsource=journal,final]
    %   \step[fieldset=eprint, null]
    % }
    % \map[overwrite]{
    %   \pertype{inproceedings}
    %   \step[fieldsource=booktitle,final]
    %   \step[fieldset=eprint, null]
    % }
  }
}

\addbibresource{biblio-cv-subs.bib}

\pagestyle{fancy}
\fancyhf{}
\rhead{\scshape Danica Sutherland -- 590329}
\cfoot{\thepage}

\begin{document}
\nocite{*}

\section{Most significant contributions to research}

\subsection{Optimized kernels for two-sample tests} \label{sec:testing}

In \cite{sutherland:opt-mmd}, we proposed a method to find the most powerful kernel for a two-sample test. (I was the lead author on this work.) This problem is extremely widespread, and to my knowledge this method is in general domains still essentially the best-performing method known for this task today. We revisited the method in \cite{liu:deep-testing}, suggesting some tweaks to the estimator which allowed for more scalable learning of deep kernels as well as providing theoretical proofs that the method works. (I was the last author on this paper, but also wrote most of the proofs.)
Our follow-up \cite{liu:meta-2st} built off this scheme to more efficiently handle situations where several related testing problems are available,
and our current submission \cite{deka:mmd-bfair} extends this technique to also allow for \emph{minimizing} the power of some tests to build a technique that learns representations fair with respect to some protected attribute.
(I played the ``advisor'' role and was last author on these projects.)

Our tutorial at NeurIPS 2019, which was popular enough that we were upgraded to a larger room the morning of the tutorial, helped spread these and related ideas to a large audience. For instance, we've been told that a top Formula 1 company is using methods learned from our tutorial to help design new cars.

\subsection{Uniform convergence of interpolators} \label{sec:interpolators}
In 2019, Nagarajan and Kolter brought considerable doubt as to whether the standard workhorse of statistical learning theory, uniform convergence, is capable of explaining learning in certain high-dimensional regimes. Separately, several other researchers, in particular Mikhail Belkin, were advocating a point of view that ``there are no'' uniform convergence bounds capable of explaining modern interpolating machine learning methods, ``and no reason they should exist.''

In \cite{zhou:uniform-interpolation}, we suggested an alternative to simply throwing out the vast majority of thirty years of research in the field: considering uniform convergence of predictors with zero training error. We showed that in one particular high-dimensional linear regression setting, this style of analysis can appropriately show consistency of the minimal-norm linear interpolator, and in fact can analyze the behaviour of larger-norm interpolators as well. (I joined this project after it began, but played the ``week-to-week advisor'' role as well as writing some of the proofs myself.) This paper spawned a very direct sequel from an unrelated research group (``Exact Gap between Generalization Error and Uniform Convergence in Random Feature Models'' by Yang et al.) and has already been cited in major recent review papers and an online textbook on deep learning theory.

Our follow-up \cite{koehler:gaussian-interpolators} extended the results of the previous paper from a single particular learning problem to any Gaussian linear regression problem, matching previous conditions for consistency of the minimal-norm linear interpolator and confirming a bound speculated by the earlier paper for larger-norm interpolators.
(It was one of under 1\% of submissions to receive a prestigious oral presentation at NeurIPS.)
Our submission \cite{zhou:optimistic-rates} extends the technique beyond covering only exact interpolators, to explain the behaviour of ``near-interpolators''.
Our follow-up \Cite{zhou:moreau} further brings these results beyond linear regression,
giving new explanations of several categories of generalized linear models
and gives novel results about, for instance, the properties of max-margin classifiers in certain settings.
(I mostly had an ``advisor''-type role on these projects.)


\subsection{Generative model evaluation} \label{sec:kid}

As a part of \cite{binkowski:mmd-gans}, I – and this part of the paper was almost entirely my personal contribution – contributed significantly to the state of evaluation of generative models. First, I pointed out that the standard evaluation metric for GANs, the Fréchet Inception Distance (FID), has an estimator with very unusual properties: it is strongly biased but has a very small variance. Practitioners, used to unbiased estimators, would then frequently think "oh, I see almost no variance with a small sample size, it must be basically the true value" – but I demonstrated that it is very easy to find examples where model A looks far superior to model B with a moderate sample size, while under the true population criterion model B is actually much better. Although FID has remained mostly standard, practitioners at least mostly use large and consistent sample sizes for comparison now.

I also proposed a similar metric with an unbiased, asymptotically normal estimator, the Kernel Inception Distance (KID). The KID is now included in the standard TensorFlow GAN toolkit, is included in some new significant evaluation efforts such as the HYPE benchmark, and received the endorsement of Ian Goodfellow, the inventor of GANs.

Our current submission \cite{shirzad:contrastive-graph-eval} develops a closely related technique for evaluating graph generative models, which have some very different characteristics than generating natural images. This work, done by my student and an outside collaborator with my supervision, improves the features used to evaluate graph models.


\subsection{Theory of Invariant Risk Minimization} \label{sec:irm}
The paper ``Invariant Risk Minimization'' of Arjovsky et al.\ caused quite a splash, proposing a new paradigm of learning with the promise to be far more robust to ``spurious correlations,'' along with a reasonably-practical approximate algorithm. Yet, when practitioners tried out the algorithm on various problems, results were mostly disappointing. We initially began the project that became \cite{kamath:irm} with the aim of a different approximation to the IRM framework, but eventually realized that there were many pressing questions about whether the framework itself even worked. This paper (where I was the ``week-to-week advisor'' and proved some results) addressed several core questions about what the IRM framework can and cannot do. It received an oral presentation at AISTATS 2021, substantially clarified what is possible in the IRM framework, and fairly conclusively showed that the particular algorithm ``IRMv1'' is fundamentally flawed even in surprisingly simple situations.


\subsection{MMD GANs}

Work during my postdoc significantly advanced the understanding and practice of MMD-based GAN models. We began \cite{binkowski:mmd-gans}  to correct some misleading theoretical claims and suboptimal practical choices in prior approaches to MMD GANs. Our paper clarified the relation of previous approaches to the overall GAN framework and proposed an updated regularization scheme as well as various other practical fixes that yielded substantially improved models.
(For ``Demystifying,'' I was co-first author; I led the work on evaluation methods described above as well as parts of the theoretical analysis, and played a more advisory role on the experiments.)

\cite{arbel:smmd} further improved the situation of MMD GANs by identifying a foundational issue with theory, demonstrating that they failed too often even on trivial toy problems, then defining a new notion of a distance that solved the failure theoretically, then found that it significantly improved real models. Although the model hasn't taken off to the extent that I might have hoped for, the theory-to-practice takeaway of this paper was deeply satisfying to me, and it remains a competitive GAN variant today. (On this paper I was again co-first author; I led on aspects of the theoretical analysis and positioning relative to other work, including experiments investigating the behaviour of less-practical variants of our proposal, and played an advisory role for the main experiments.)


\clearpage
\section{Additional Information on Contributions}
\renewcommand{\bibeqcon}{{\footnotemark[1]}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}  %\fnsymbol{footnote}}
Machine learning most values publications at NeurIPS and ICML, followed closely by the more specialized conferences ICLR, AISTATS, UAI, COLT, and KDD. The vast majority of my work has been at these venues. Each conducts double-blind reviewing with at least three reviewers, and typically have acceptance rates around 20-30\%.
At NeurIPS and AISTATS, additionally perhaps 10\% of submissions receive spotlights, and a few percent get oral presentations: I have received several of these distinctions, including a NeurIPS oral for the paper \cite{koehler:gaussian-interpolators} at NeurIPS 2021 (0.6\% of submissions).
% Journal publications are for the most part considered far less important.

Author order is typically meaningful,
roughly with students and postdocs first sorted by decreasing contribution,
then professors in increasing order of seniority and contribution,
such that the most prestigious positions are first and last.
Shared first authorships, marked with a star, are also common.
As a postdoc, I shared co-first authorship on many papers, with:
\begin{itemize}[nosep]
\item Junier Oliva: "Linear-time Learning on Distributions with Approximate Kernel Embeddings", AAAI 2016
\item Ho Chung Leon Law: "Bayesian Approaches to Distribution Regression", AISTATS 2018
\item Heiko Strathmann: "Efficient and principled score estimation with Nyström kernel exponential families", AISTATS 2018
\item Mikołaj Bińkowski: "Demystifying MMD GANs", ICLR 2018
\item Michael Arbel: "On gradient regularizers for MMD GANs", NeurIPS 2018
\item Li Wenliang: "Learning deep kernels for exponential family densities", ICML 2019
\end{itemize}
On the first two of these papers, the work was truly equally split. For the last four, I mostly took a more advisory role on the implementation, but a more primary role in the theoretical development and writing components.

I have also spent a significant amount of time over the past several years in peer review, including repeated time serving as a reviewer for conferences including NeurIPS and ICML (each of which have awarded me Best Reviewer-type awards), ICLR, AISTATS, AAAI, COLT, and SoCG, as well as serving as a meta-reviewer (Area Chair or Senior Program Committee) repeatedly for NeurIPS, ICML, AISTATS, and AAAI. I have also done many journal reviews for JMLR, Springer MLJ, Bernoulli, IEEE T-PAMI, IEEE TSP, and Comptes rendus, and served on thesis committees for a University of Cambridge MPhil and a Ghent University PhD as well as a UBC MSc.

\clearpage
\section{Contributions to the Training of Highly Qualified Personnel}
I currently advise three PhD and three MSc students at UBC.
% I began at UBC in January 2021, but I admitted and began advising four UBC students (two MSc, two PhD) in September 2020. Another MSc student switched to my supervision in January 2021, and another PhD began in September 2021.
%
Each of the PhD students came in with some ideas of areas they wanted to work on,
and I think through my interactions with them we've been able to combine their initial interests with mine in a way yielding something neither of us would have done on our own.
Yi Ren came in excited about neural iterated learning frameworks:
talking about that problem brought us to an interesting analysis of how ``knowledge'' from a teacher's labels is used in a student's representation learning process,
and ongoing work on improving transfer learning.
The same happened with Wonho Bae
(whose work on weakly-supervised object localization led naturally into a study of active learning and now on warm-starting in deep networks)
% Wonho Bae, similarly, came in with expertise in weakly-supervised object localization, and our discussions there led to interesting new approaches for representation learning in that problem (two papers),
% but also to a novel method for active learning based on neural tangent kernels
% and ongoing work on cold- versus warm-started optimization in deep networks.
and Hamed Shirzad (improving the rigour of generative models of graphs starting with evaluation metrics -- see \cref{sec:kid}).

The MSc students have naturally needed a little more guidance.
With Namrata Deka and Milad Jalali, each working on projects related to statistical testing I brought up,
I've learned a lot about balancing between hands-on guidance and letting them explore on their own;
each has now finished or almost finished those projects with their own spins,
as well as other collaborative projects.
% Namrata now has a paper under submission (see \cref{sec:testing}) and a collaborative closely-related followup,
% while Milad's original project should be submitted soon, along with another recent collaborative submission.
Mohamad Amin Mohamadi first started following up on \cref{sec:irm},
and rapidly developed from missing a lot of background to being able to critically understand difficult contemporary theory in only a few months.
We put that work on hold, though, for him to investigate neural tangent kernels in active learning;
after reading and internalizing a lot of difficult theoretical material,
he proved the results we needed for that study,
and led a foray into complex details of a far more careful empirical study than the norm in the area.
This brought him to a conceptual breakthrough in understanding a far faster approximation to the emprical neural tangent kernel in classification settings (on arXiv and submitting soon), plus exciting ongoing work.
This has shown me the importance of finding the right project:
at first he struggled, but with the right project he's flourished.
% I may well have written him off had the circumstances of his supervisor switch been slightly different,
% but instead he's flourished.

Prior to starting at UBC,
I also had semi-formal advising relationships with two students at TTI-Chicago,
both advised by Nati Srebro. %, who asked for my help when he found himself with significantly increased childcare responsibilities in March 2020.
I continue to have weekly advising meetings with one, Lijia Zhou, resulting in a very productive line of work (\cref{sec:interpolators}).
The other,
Akilesh Tangella,
worked on the paper of \cref{sec:irm}
but left for industry afterwards.
I had many conversations with him leading to this decision, which hopefully helped;
% I hope I helped him navigate his career path more smoothly;
when I asked for consent to mention his name in NSERC applications, he replied, ``Of course!!! You have helped me a lot in life.''

This kind of graduate student support is one important aspect of building a fair, effective, and humane environment for students. Another has been made clear in my involvement in graduate admissions. The applicant pool is, of course, vast majority white or Asian, mostly men, and heavily weighted towards those with relative economic privilege; the ways we identify top candidates from the enormous pool introduce their own biases on top of that. For one student I admitted, who went to a relatively unknown undergraduate institution, I had to provide additional justification to the university -- an indication that even that level of diversity in admissions is fairly rare. I will continue to actively work towards identifying excellent applicants who may not seem to be the most enormously qualified ``on paper,''
and have also worked on Queer in AI's grad school application support program to help applicants around the world.

% I have also been working to fight another EDI challenge closer to my own experience.
My own experience has also highlighted another EDI challenge.
As a student, like many who eventually became academics, I identified strongly with my academic and intellectual pursuits.
Unlike most such students, though, I was also struggling to come to terms with my own trans identity.
Not seeing any signs around me that it was possible for trans or even gay people to have a career in this field, it felt impossible to explore without abandoning my life path.
For those less able to ignore or hide their ``competing'' identities, it is extremely easy to see how they could be driven out of the field even without anyone being intentionally antagonistic.
I've worked towards fighting this as a core organizer of Queer in AI, and through UBC's Queer Coded student group; both approaches hopefully help queer students see that computer science and machine learning are options for them too.


\end{document}
{%- endraw -%}
