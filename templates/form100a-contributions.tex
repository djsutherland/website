{% raw -%}
\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[american]{babel}
\usepackage{csquotes}

\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage[margin=0.75in,headheight=13.6pt]{geometry}
\usepackage{times}
\usepackage[compact]{titlesec}

\usepackage{todonotes}
% \usepackage{hyperref}

\usepackage[capitalize,noabbrev]{cleveref}

\usepackage[maxbibnames=20,doi=false,url=false,sorting=ydnt,style=authortitle,giveninits=true,eprint=false,uniquelist=false,dashed=false]{biblatex}
\DeclareNameAlias{author}{given-family}

% \DeclareFieldFormat[unpublished,misc]{title}{\mkbibquote{#1\isdot}}
% \DeclareFieldFormat[unpublished,misc]{labeltitle}{\mkbibquote{#1}}
\renewbibmacro{in:}{}

% cite by title only
\renewbibmacro{cite}{\usebibmacro{cite:title}}

\newcommand{\bibeqcon}{{\footnotemark[1]}}

% bold student names in bib
\renewcommand*{\mkbibnamegiven}[1]{%
  \ifitemannotation{me}%
    {#1}%{\textbf{#1}}%
    {\ifitemannotation{mystudent}{\textbf{#1}}{#1}}}
\renewcommand*{\mkbibnamefamily}[1]{%
  \ifitemannotation{me}%
    {#1}%{\textbf{#1}}%
    {\ifitemannotation{mystudent}{\textbf{#1}}{#1}}%
  \ifitemannotation{equal}
    {\bibeqcon}
    {}%
}

{% endraw %}

{%- set sections = [
  ["Journal papers", ["journal"], None],
  ["Low-acceptance-rate conference papers", ["conference"], None],
  ["Papers under submission to journals and low-acceptance-rate conferences", ["preprint", "private"], True],
  ["High-acceptance-rate and non-refereed contributions", ["workshop", "poster", "tech-report", "preprint"], False],
] -%}

{% for sec_name, sec_types, submitted_only in sections %}
  \DeclareBibliographyCategory{sec{{ loop.index }}}
  \addtocategory{sec{{ loop.index -}} }{
    {%- for paper in venue_type_map[sec_types]
        if paper|in_last_years(6) and not paper.full_version
           and (submitted_only is none or (paper.submitted and submitted_only or not paper.submitted and not submitted_only))
    -%}
      {{- paper | bibtex_key(coauthors) -}}
      {%- if not loop.last -%},{%- endif -%}
    {%- endfor -%}
  }
{% endfor %}

{% raw %}

\DeclareSourcemap{
  \maps[datatype=bibtex]{
    \map[overwrite]{
      \step[fieldset=pubstate, null]
    }
    % % arxiv info only if there's no venue
    % \map[overwrite]{
    %   \pertype{article}
    %   \step[fieldsource=journal,final]
    %   \step[fieldset=eprint, null]
    % }
    % \map[overwrite]{
    %   \pertype{inproceedings}
    %   \step[fieldsource=booktitle,final]
    %   \step[fieldset=eprint, null]
    % }
  }
}

\addbibresource{biblio-cv-subs.bib}

\pagestyle{fancy}
\fancyhf{}
\rhead{\scshape Danica Sutherland -- 590329}
\cfoot{\thepage}

\begin{document}
\nocite{*}

\section{Most significant contributions to research}

\subsection{Optimized kernels for understanding of distributions} \label{sec:testing}

A line of my work since my postdoc has focused on kernel methods for understanding distributions.
One major area is two-sample testing,
where we attempt to tell whether two datasets are significantly different from each other,
or might have been produced from the same underlying distribution and differ only by chance.
This problem is extremely widespread,
and the $t$-tests or Kolmogorov-Smirnov tests solving simple variants of this problem
are among the most common tools used by scientists.
In \cite{sutherland:opt-mmd} (where I was lead author)
and then followed up in \cite{liu:deep-testing} (where I was last author and did most of the theoretical analysis),
we proposed a method to expand the applicability to far more complex domains,
such as directly comparing distributions of natural images.
We later built off this scheme to more efficiently handle situations where several related testing problems are available (\cite{liu:meta-2st}; I was the last-author ``advisor'').

A closely connected problem is that of measuring (conditional) independence.
\cite{li:ssl-hsic} used optimized kernels to maximize the dependence between images and their features in self-supervised learning
(and argued this was a better viewpoint than mutual information for existing approaches).
\Cite{deka:mmd-bfair} instead minimized conditional dependence for fair representation learning,
finding features or predictions which are conditionally independent of a (discrete) protected attribute given a (discrete) target.
\cite{pogodin:circe} took a similar approach for continuous targets and protected attributes.
This is highly relevant for intersectional fairness settings where the number of discrete attributes is too large to handle with discrete approaches,
or e.g.\ being fair with respect to an image of a loan applicant.
It is also used for settings such as out-of-domain generalization,
e.g.\ finding visual features which do not depend on the time of day.
(I played an ``advisor'' role in all of these projects.)

Our tutorial at NeurIPS 2019, which was popular enough that we were upgraded to a larger room the morning of the tutorial, helped spread these and related ideas to a large audience. For instance, we've been told that a top Formula 1 company is using methods learned from our tutorial to help design new cars.

\subsection{Scalable graph transformers} \label{sec:exphormers}
Transformers have been the breakout success story in machine learning over the past five years,
but their application to graph neural networks has lagged applications in sequence and image modeling,
in part because their quadratic dependence on the number of nodes in the graph
is infeasible for large graphs.
Our paper \cite{shirzad:exphormer} %(a collaboration of my student and myself with researchers at Google)
used the theoretical construct known as expander graphs
to efficiently handle larger graphs.
This approach, in addition to its theoretical justification,
has excellent empirical performance,
setting a new state of the art for several well-studied datasets.
Our work was highlighted in several blog posts on recent progress in graph networks.
We are currently extending the approach to scale to even larger graphs.

\subsection{Uniform convergence of interpolators} \label{sec:interpolators}
In 2019, Nagarajan and Kolter brought considerable doubt as to whether the standard workhorse of statistical learning theory, uniform convergence, is capable of explaining learning in certain high-dimensional regimes. Separately, several other researchers, in particular Mikhail Belkin, were advocating a point of view that ``there are no'' uniform convergence bounds capable of explaining modern interpolating machine learning methods, ``and no reason they should exist.''

In \cite{zhou:uniform-interpolation}, we suggested an alternative to simply throwing out the vast majority of thirty years of research in the field: considering uniform convergence of predictors with zero training error. We showed that in one particular high-dimensional linear regression setting, this style of analysis can appropriately show consistency of the minimal-norm linear interpolator, and in fact can analyze the behaviour of larger-norm interpolators as well. (I joined this project after it began, but played the ``week-to-week advisor'' role as well as writing some of the proofs myself.) This paper spawned a very direct sequel from an unrelated research group (``Exact Gap between Generalization Error and Uniform Convergence in Random Feature Models'' by Yang et al.) and has been cited in major recent review papers and an online textbook on deep learning theory.

Our follow-up \cite{koehler:gaussian-interpolators} extended the results of the previous paper from a single particular learning problem to any Gaussian linear regression problem, matching previous conditions for consistency of the minimal-norm linear interpolator and confirming a bound speculated by the earlier paper for larger-norm interpolators.
(It was one of under 1\% of submissions to receive a prestigious oral presentation at NeurIPS.)
Our paper \cite{zhou:optimistic-rates} extends the technique beyond covering only exact interpolators, to explain the behaviour of ``near-interpolators''.
Finally, \cite{zhou:moreau} further brings these results beyond linear regression,
giving new explanations of several categories of generalized linear models
and gives novel results about, for instance, the properties of max-margin classifiers in certain settings.
(I mostly had an ``advisor''-type role on these projects.)

%\subsection{Fast approximations to empirical neural tangent kernels} \label{sec:pntk}


\subsection{Generative model evaluation} \label{sec:kid}

As a part of \cite{binkowski:mmd-gans}, I – and this part of the paper was almost entirely my personal contribution – contributed significantly to the state of evaluation of generative models. First, I pointed out that the standard evaluation metric for GANs, the Fréchet Inception Distance (FID), has an estimator with very unusual properties: it is strongly biased but has a very small variance. Practitioners, used to unbiased estimators, would then frequently think "oh, I see almost no variance with a small sample size, it must be basically the true value" – but I demonstrated that it is very easy to find examples where model A looks far superior to model B with a moderate sample size, while under the true population criterion model B is actually much better. Although FID has remained mostly standard, practitioners at least mostly use large and consistent sample sizes for comparison now.
I also proposed a similar metric with an unbiased, asymptotically normal estimator, the Kernel Inception Distance (KID),
which has been included in several standard GAN toolkits
%. The KID is now included in the standard TensorFlow GAN toolkit, is included in some new significant evaluation efforts such as the HYPE benchmark,
and received the endorsement of Ian Goodfellow, the inventor of GANs.

Our paper \cite{shirzad:contrastive-graph-eval} develops a closely related technique for evaluating graph generative models, which have some very different characteristics than generating natural images. This work, done by my student and an outside collaborator with my supervision, improves the features used to evaluate graph models.


\subsection{Theory of Invariant Risk Minimization} \label{sec:irm}
The paper ``Invariant Risk Minimization'' of Arjovsky et al.\ caused quite a splash, proposing a new paradigm of learning with the promise to be far more robust to ``spurious correlations,'' along with a reasonably-practical approximate algorithm. Yet, when practitioners tried out the algorithm on various problems, results were mostly disappointing. We initially began the project that became \cite{kamath:irm} with the aim of a different approximation to the IRM framework, but eventually realized that there were many pressing questions about whether the framework itself even worked. This paper (where I was the ``week-to-week advisor'' and proved some results) addressed several core questions about what the IRM framework can and cannot do. It received an oral presentation at AISTATS 2021, substantially clarified what is possible in the IRM framework, and fairly conclusively showed that the particular algorithm ``IRMv1'' is fundamentally flawed even in surprisingly simple situations.


% \subsection{MMD GANs}
% 
% Work during my postdoc significantly advanced the understanding and practice of MMD-based GAN models. We began \cite{binkowski:mmd-gans}  to correct some misleading theoretical claims and suboptimal practical choices in prior approaches to MMD GANs. Our paper clarified the relation of previous approaches to the overall GAN framework and proposed an updated regularization scheme as well as various other practical fixes that yielded substantially improved models.
% (For ``Demystifying,'' I was co-first author; I led the work on evaluation methods described above as well as parts of the theoretical analysis, and played a more advisory role on the experiments.)
% 
% \cite{arbel:smmd} further improved the situation of MMD GANs by identifying a foundational issue with theory, demonstrating that they failed too often even on trivial toy problems, then defining a new notion of a distance that solved the failure theoretically, then found that it significantly improved real models. Although the model hasn't taken off to the extent that I might have hoped for, the theory-to-practice takeaway of this paper was deeply satisfying to me, and it remains a competitive GAN variant today. (On this paper I was again co-first author; I led on aspects of the theoretical analysis and positioning relative to other work, including experiments investigating the behaviour of less-practical variants of our proposal, and played an advisory role for the main experiments.)


\clearpage
\section{Additional Information on Contributions}
\renewcommand{\bibeqcon}{{\footnotemark[1]}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}  %\fnsymbol{footnote}}
Machine learning most values publications at NeurIPS and ICML, followed closely by the more specialized conferences ICLR, AISTATS, UAI, and COLT. The vast majority of my work has been at these venues, or top venues in closely related fields such as computer vision. Each conducts double-blind reviewing with at least three reviewers, and typically have acceptance rates around 20-30\%.
Perhaps 10\% of submissions receive ``spotlight presentations'' at most of these conferences, and up to a few percent get oral presentations: I have received several of these distinctions, including a NeurIPS oral for the paper \cite{koehler:gaussian-interpolators} at NeurIPS 2021 (0.6\% of submissions).

Author order is typically meaningful,
roughly with students and postdocs first sorted by decreasing contribution,
then professors in increasing order of seniority and contribution,
such that the most prestigious positions are first and last.
Several of my students' papers use this marker.
In my collaborations with other researchers where I am not listed last,
this has sometimes been because we played similar roles in the work and they came last out of seniority,
and sometimes because my overall contribution to the work was smaller than theirs.
(This latter situation was particularly the case for
\cite{seo:contrastive-wsod}.)
An exception to these general rules is on very-large authorship lists,
particularly \cite{flamary:pot} and \cite{queerinai:qai};
here ordering outside of the first author was essentially arbitrary.

Shared first authorships, marked with a star, are also common.
As a student or postdoc, I shared co-first authorship on six papers.
%\begin{itemize}[nosep]
%\item Junier Oliva: \cite{sutherland:hdd}, AAAI 2016
%\item Ho Chung Leon Law: \cite{law:bdr}, AISTATS 2018
%\item Heiko Strathmann: \cite{sutherland:kexpfam-nystroem}, AISTATS 2018
%\item Mikołaj Bińkowski: \cite{binkowski:mmd-gans}, ICLR 2018
%\item Michael Arbel: \cite{arbel:smmd}, NeurIPS 2018
%\item Li Wenliang: \cite{wenliang:dkef}, ICML 2019
%\end{itemize}
For two (with J Oliva [AAAI 2016] and HCL Law [AISTATS 2018]),
the work was truly equally split.
For another four (with H Strathmann [AISTATS 2018], M Bińkowski [ICLR 2018], M Arbel [NeurIPS 2018], and L Wenliang [ICML 2019]),
I mostly took a more advisory role on the implementation, but a more primary role in the theoretical development and writing components.

In addition to research and regular service as an Area Chair or reviewer at many top venues in my and related fields,
I have also helped advance research through my involvement with
Queer in AI (mentoring and removing barriers for other queer researchers)
and the Name Change Policy Working Group (improving the state of academic publishing for other trans researchers).
% I have also spent a significant amount of time over the past several years in peer review, including repeated time serving as a reviewer for conferences including NeurIPS and ICML (each of which have awarded me Best Reviewer-type awards), ICLR, AISTATS, AAAI, COLT, and SoCG, as well as serving as a meta-reviewer (Area Chair or Senior Program Committee) repeatedly for NeurIPS, ICML, AISTATS, and AAAI. I have also done many journal reviews for JMLR, Springer MLJ, Bernoulli, IEEE T-PAMI, IEEE TSP, and Comptes rendus, and served on thesis committees for a University of Cambridge MPhil and a Ghent University PhD as well as a UBC MSc.

\clearpage
\section{Contributions to the Training of Highly Qualified Personnel}

I have four current PhD students at UBC
(fourth years: Yi Ren, Wonho Bae, third: Hamed Shirzad, first: Nathaniel Xu),
as well as one MSc student on the ``PhD track'' (first year: Zheng He).
The senior students each have two or three top-tier (co-)first-author publications from their time here.

I have previously graduated three MSc students.
Namrata Deka has since begun a PhD program in the Machine Learning Department at Carnegie Mellon;
Mohamad Amin Mohamadi is waiting on a visa but will soon begin a PhD at the Toyota Technological Institute at Chicago;
Milad Jalali Asadabadi just graduated and is applying for industry jobs.
Each had two publications in their degrees.
%(Namrata and Mohamad Amin top-tier venues as [co-]first authors, Milad as middle author at respected venues),
I also advised one undergraduate summer research project (Achinth Bharadwaj),
who is trying to continue it for publication while beginning work.
I also informally worked with Lijia Zhou (UChicago Statistics) before starting at UBC and continuing for most of his PhD (\cref{sec:interpolators}); he recently graduated and now works in finance.
% Akilesh Tangella chose to leave for industry after collaborating on \cite{kamath:irm}.
% I had many conversations with him leading to this decision,
% and when I asked for consent to mention his name in NSERC applications, he replied, ``Of course!!! You have helped me a lot in life.''

% It has been a truly amazing experience to watch each of these students grow.
% Especially satisfying, perhaps, were the master's students,
% who started with me in the depths of the pandemic
% with little idea of what they wanted to do and limited background in their respective topics,
% but became capable independent researchers able to formulate and tackle complex research questions.

I have tried to actively help each of these students grow in their ability to formulate and guide research programs,
rather than just asking them to implement papers based on my ideas;
seeing my senior PhD students and especially the MSc students who've gone onto PhDs
blossom into highly capable independent researchers has been especially satisfying.
Some projects do, of course, spark from one of my suggestions;
even so, I tried to regularly emphasize asking them about the big picture,
in addition to asking questions about details of things they'd tried and immediate next steps.
Some projects also came out of discussions with students on their interests,
often combining students' suggestions with my ideas in a way yielding something neither of us would have done on their own.
For instance, Yi Ren came in with an excitement for neural iterated learning frameworks,
an idea about which I was relatively skeptical;
my confusion about why this approach worked
led to us improving basic understanding of ``component parts'' of the framework
(knowledge distillation and transfer learning)
before moving on to his most recent work on exploiting iterated learning for ``compositional'' generalization.
In all of these processes,
there have of course been difficult times and the occasional abandoned project,
but I try hard to guide them through it smoothly
and give them enough time both to learn foundations and explore things I suspect might turn out to be dead ends,
since navigating this is one of the most important things to learn in research.
(One student characterized one of my main roles in advising meetings as ``convincing me things are worthwhile when I'm depressed'' about some aspect of an ongoing project.)

This kind of graduate student support is one important aspect of building a fair, effective, and humane environment for students. Another has been made clear in my involvement in graduate admissions. The applicant pool is, of course, vast majority white or Asian, mostly men, and heavily weighted towards those with relative economic privilege; the ways we identify top candidates from the enormous pool introduce their own biases on top of that. For one student I admitted, who went to a relatively unknown undergraduate institution, I had to provide additional justification to the university -- an indication that even that level of diversity in admissions is fairly rare. I will continue to actively work towards identifying excellent applicants who may not seem to be the most enormously qualified ``on paper,''
and have also worked on Queer in AI's grad school application support program to help applicants around the world.

My own experience has also highlighted another EDI challenge.
As a student, like many who eventually became academics, I identified strongly with my academic and intellectual pursuits.
Unlike most such students, though, I was also struggling to come to terms with my own trans identity.
Not seeing any signs around me that it was possible for trans or even gay people to have a career in this field, it felt impossible to explore without abandoning my life path.
For those less able to ignore or hide their ``competing'' identities, it is extremely easy to see how they could be driven out of the field even without anyone being intentionally antagonistic.
I've worked towards fighting this as a core organizer of Queer in AI, and through involvement with UBC's Queer Coded student group; both approaches hopefully help queer students see that computer science and machine learning are options for them too.

\end{document}
{%- endraw -%}
